{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from tabulate import tabulate\n",
    "from torch import optim\n",
    "import torchvision.utils\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "\n",
    "import config\n",
    "from utils import imshow\n",
    "from models import SiameseNetwork, CNN\n",
    "from training import trainSiamese,inferenceSiamese, trainCNN\n",
    "from datasets import SiameseNetworkDataset, CNNDataset,generate_csv_compare\n",
    "from loss_functions import ContrastiveLoss\n",
    "\n",
    "\n",
    "if not os.path.exists('state_dict'):\n",
    "    os.makedirs('state_dict')\n",
    "    \n",
    "# sys.stdout = open(os.devnull, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num per class: 4\n",
      "Data directory:  ../../datasets/AT&T Database of Faces/faces/training\n",
      "Data directory:  ../../datasets/AT&T Database of Faces/faces/training\n",
      "Data directory:  ../../datasets/AT&T Database of Faces/faces/training\n",
      "Data directory:  ../../datasets/AT&T Database of Faces/faces/training\n",
      "accuracy siamese [[0.5714285714285714, 0.6857142857142857, 0.5714285714285714, 0.6571428571428571]]\n",
      "accuracy CNN [[0.5142857142857142, 0.17142857142857143, 0.2857142857142857, 0.6857142857142857]]\n",
      "num per class: 5\n",
      "Data directory:  ../../datasets/AT&T Database of Faces/faces/training\n",
      "Data directory:  ../../datasets/AT&T Database of Faces/faces/training\n",
      "Data directory:  ../../datasets/AT&T Database of Faces/faces/training\n",
      "Data directory:  ../../datasets/AT&T Database of Faces/faces/training\n",
      "accuracy siamese [[0.5714285714285714, 0.6857142857142857, 0.5714285714285714, 0.6571428571428571], [0.45714285714285713, 0.6857142857142857, 0.7142857142857143, 0.7428571428571429]]\n",
      "accuracy CNN [[0.5142857142857142, 0.17142857142857143, 0.2857142857142857, 0.6857142857142857], [0.6857142857142857, 0.5428571428571428, 0.5714285714285714, 0.8285714285714286]]\n",
      "num per class: 6\n",
      "Data directory:  ../../datasets/AT&T Database of Faces/faces/training\n",
      "Data directory:  ../../datasets/AT&T Database of Faces/faces/training\n",
      "Data directory:  ../../datasets/AT&T Database of Faces/faces/training\n",
      "Data directory:  ../../datasets/AT&T Database of Faces/faces/training\n",
      "accuracy siamese [[0.5714285714285714, 0.6857142857142857, 0.5714285714285714, 0.6571428571428571], [0.45714285714285713, 0.6857142857142857, 0.7142857142857143, 0.7428571428571429], [0.7428571428571429, 0.7714285714285715, 0.6857142857142857, 0.6285714285714286]]\n",
      "accuracy CNN [[0.5142857142857142, 0.17142857142857143, 0.2857142857142857, 0.6857142857142857], [0.6857142857142857, 0.5428571428571428, 0.5714285714285714, 0.8285714285714286], [0.6571428571428571, 0.8, 0.6857142857142857, 0.6571428571428571]]\n",
      "num per class: 7\n",
      "Data directory:  ../../datasets/AT&T Database of Faces/faces/training\n"
     ]
    }
   ],
   "source": [
    "accS = []\n",
    "accC = []\n",
    "nums = np.arange(4,10)\n",
    "for num in nums:\n",
    "    print(\"num per class:\",num)\n",
    "    \n",
    "    accS_temp = []\n",
    "    accC_temp = []\n",
    "    for r in range(1,5):\n",
    "        generate_csv_compare(config.training_dir,config.compare_siamese_csv,config.compare_cnn_csv,config.compare_test_csv,\n",
    "                             num_per_class = num)\n",
    "\n",
    "        # Split the dataset into train and validation sets\n",
    "        siamese_dataset = SiameseNetworkDataset(config.compare_siamese_csv,\n",
    "                                                transform=transforms.Compose([\n",
    "                                                    transforms.Resize((config.img_height,config.img_width)),\n",
    "                                                    transforms.ToTensor(),\n",
    "                                                    transforms.Normalize(0,1)]),\n",
    "                                                should_invert=False)\n",
    "\n",
    "        num_train = round(0.9*siamese_dataset.__len__())\n",
    "        num_validate = siamese_dataset.__len__()-num_train\n",
    "\n",
    "\n",
    "\n",
    "        siamese_train, siamese_valid = random_split(siamese_dataset, [num_train,num_validate])\n",
    "\n",
    "        siamese_train_dataloader = DataLoader(siamese_train,\n",
    "                                shuffle=True,\n",
    "                                num_workers=8,\n",
    "                                batch_size=config.train_batch_size)\n",
    "\n",
    "        siamese_valid_dataloader = DataLoader(siamese_valid,\n",
    "                                shuffle=True,\n",
    "                                num_workers=8,\n",
    "                                batch_size=1)\n",
    "\n",
    "        # Training\n",
    "        netS = SiameseNetwork().cuda()\n",
    "        criterionS = ContrastiveLoss()\n",
    "        optimizer = optim.Adam(netS.parameters(),lr = config.learning_rate )\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer,config.step_size, config.gamma)\n",
    "\n",
    "        netS, train_loss_historyS, valid_loss_historyS,dict_nameS = trainSiamese(netS,criterionS,optimizer,scheduler,siamese_train_dataloader,\n",
    "                     siamese_valid_dataloader,config.train_number_epochs,do_show=False,do_print=False)\n",
    "\n",
    "        ## Train CNN with Cross Entropy Loss\n",
    "\n",
    "        cnn_dataset = CNNDataset(config.compare_cnn_csv,\n",
    "                                                transform=transforms.Compose([\n",
    "                                                    transforms.Resize((config.img_height,config.img_width)),\n",
    "                                                    transforms.ToTensor(),\n",
    "                                                    transforms.Normalize(0,1)]),\n",
    "                                                should_invert=False)\n",
    "\n",
    "        cnn_train_dataloader = DataLoader(cnn_dataset,\n",
    "                                shuffle=True,\n",
    "                                num_workers=8,\n",
    "                                batch_size=config.train_batch_size)\n",
    "\n",
    "        netC = CNN().cuda()\n",
    "        criterionC = CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(netC.parameters(),lr = config.learning_rate )\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer,config.step_size, config.gamma)\n",
    "\n",
    "        netC, train_loss_historyC, dict_nameC = trainCNN(netC,criterionC,optimizer,scheduler,cnn_train_dataloader,\n",
    "                     config.train_number_epochs,do_show=False,do_print=False)\n",
    "\n",
    "        test_dataset = CNNDataset(config.compare_test_csv,\n",
    "                                                transform=transforms.Compose([\n",
    "                                                    transforms.Resize((config.img_height,config.img_width)),\n",
    "                                                    transforms.ToTensor(),\n",
    "                                                    transforms.Normalize(0,1)]),\n",
    "                                                should_invert=False)\n",
    "\n",
    "        test_dataloader = DataLoader(test_dataset,\n",
    "                                shuffle=False,\n",
    "                                num_workers=1,\n",
    "                                batch_size=1)\n",
    "        true_labels = []\n",
    "        for _,label in iter(test_dataloader):\n",
    "            true_labels.append(int(label))\n",
    "\n",
    "        netS = SiameseNetwork().cuda()\n",
    "        netS.load_state_dict(torch.load(os.path.join(\"state_dict\",dict_nameS)))\n",
    "        netS.eval()\n",
    "\n",
    "        siamese_precalculate_dataloader = DataLoader(cnn_dataset,\n",
    "                                shuffle=False,\n",
    "                                num_workers=1,\n",
    "                                batch_size=1)\n",
    "\n",
    "        feature_vectors = []\n",
    "        temp = []\n",
    "        current_label = 0 #int(next(iter(siamese_precalculate_dataloader))[1])\n",
    "        for data,label in iter(siamese_precalculate_dataloader):\n",
    "            v = (netS.forward_once(data.cuda()).detach().cpu().numpy())[0]\n",
    "            if label==current_label:\n",
    "                temp.append(v)\n",
    "            else:\n",
    "                current_label = label\n",
    "                feature_vectors.append(deepcopy(temp))\n",
    "                temp = []\n",
    "                temp.append(v)\n",
    "        feature_vectors.append(deepcopy(temp))\n",
    "\n",
    "        inferenceS = []\n",
    "        for data, label in iter(test_dataloader):\n",
    "            current_feature = netS.forward_once(data.cuda())\n",
    "            dissims = []\n",
    "            temp = 0\n",
    "            for vectors in feature_vectors:\n",
    "                for feature in vectors:\n",
    "                    temp += F.pairwise_distance(current_feature,torch.tensor(feature).cuda()).detach().cpu().numpy()\n",
    "                dissims.append(temp / len(vectors))\n",
    "                temp = 0\n",
    "            inferenceS.append(np.argmin(dissims))\n",
    "\n",
    "        netC = SiameseNetwork().cuda()\n",
    "        netC.load_state_dict(torch.load(os.path.join(\"state_dict\",dict_nameC)))\n",
    "        netC.eval()\n",
    "\n",
    "        inferenceC = []\n",
    "        for data, label in iter(test_dataloader):\n",
    "            current_feature = netC.forward_once(data.cuda())\n",
    "            inferenceC.append(np.argmax(current_feature.detach().cpu().numpy()))\n",
    "\n",
    "        accS_temp.append(sum([true_labels[i]==inferenceS[i] for i in range(len(true_labels))])/len(true_labels))\n",
    "        accC_temp.append(sum([true_labels[i]==inferenceC[i] for i in range(len(true_labels))])/len(true_labels))\n",
    "    accS.append(deepcopy(accS_temp))\n",
    "    accC.append(deepcopy(accC_temp))\n",
    "    print(\"accuracy siamese\",accS)\n",
    "    print(\"accuracy CNN\",accC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accS_mean = [np.mean(t) for t in accS]\n",
    "accS_std = [np.std(t) for t in accS]\n",
    "accC_mean = [np.mean(t) for t in accC]\n",
    "accC_std = [np.std(t) for t in accC]\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.errorbar(nums,accS_mean, yerr=accS_std, fmt='-o',capsize=3)#, color='black',\n",
    "             #ecolor='lightgray', elinewidth=3, capsize=0);\n",
    "plt.errorbar(nums,accC_mean, yerr=accC_std, fmt='-o', capsize=3)#, color='black',\n",
    "            # ecolor='lightgray', elinewidth=3, capsize=0);\n",
    "plt.xlabel(\"Number of images in each class\")\n",
    "plt.ylabel(\"Accuracy on the testing set\")\n",
    "plt.legend([\"Single CNN + Cross-entropy loss\",\"Siamese + Contrastive loss\"])\n",
    "fig.savefig('destination_path.eps', format='eps', dpi=1200)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
